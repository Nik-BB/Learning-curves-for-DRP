{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import pickle\n",
    "from importlib import reload\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import namedtuple\n",
    "import keras_tuner as kt \n",
    "codebase_path = '/data/home/wpw035/Codebase'\n",
    "sys.path.insert(0, codebase_path) #add path to my codebase models\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DRP_utils import data_preprocessing as dp_nb\n",
    "reload(dp_nb)\n",
    "from DRP_utils import model_selection as ms_nb\n",
    "reload(ms_nb)\n",
    "from DRP_utils import testing as t_nb\n",
    "reload(t_nb)\n",
    "import Data_imports as di_nb\n",
    "reload(di_nb)\n",
    "import pairs_train_test_split as tts_nb\n",
    "import Learning_curve as lc_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/wpw035/.conda/envs/tfGPUforge/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing prot values 0.386335609896865\n",
      "num non overlapping prot and target cls: 10\n",
      "num non overlapping cls: 930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((38, 8457), (38, 17417), (38, 22804), (38, 38), (345, 345))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input data\n",
    "prot, rna, phospho_ls, one_hot_cls, one_hot_drugs, ic50_df1 = di_nb.read_input_data()\n",
    "_all_cls = prot.index\n",
    "_all_drugs = ic50_df1.columns\n",
    "assert prot.shape[0] == rna.shape[0] == phospho_ls.shape[0]\n",
    "assert phospho_ls.shape[0]  == one_hot_cls.shape[0]\n",
    "prot.shape, rna.shape, phospho_ls.shape, one_hot_cls.shape, one_hot_drugs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featrue selection (FS) and data createing for each drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11583, 908), (11583, 345), 11583)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in landmark genes for fs and find landmarks that overlap with rna data\n",
    "landmark_genes = pd.read_csv(\n",
    "    f'{codebase_path}/downloaded_data_small/landmark_genes_LINCS.txt',sep='\\t')\n",
    "landmark_genes.index = landmark_genes['Symbol']\n",
    "\n",
    "overlapping_landmarks, _ = dp_nb.keep_overlapping(\n",
    "    pd.DataFrame(landmark_genes['Symbol']), rna.T)\n",
    "\n",
    "overlapping_landmarks = overlapping_landmarks['Symbol'].values\n",
    "\n",
    "#create input data for each drug\n",
    "x_all, x_drug, y_list = dp_nb.create_all_drugs(\n",
    "    rna[overlapping_landmarks], one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "x_all = x_all.astype(np.float32)\n",
    "x_drug = x_drug.astype(np.float16)\n",
    "\n",
    "#fmt index to include drug cell line paris\n",
    "cls_drugs_index = x_all.index + '::' + x_drug.index\n",
    "x_all.index = cls_drugs_index\n",
    "x_drug.index = cls_drugs_index\n",
    "y_list.index = cls_drugs_index\n",
    "\n",
    "x_all.shape, x_drug.shape, len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11583, 721)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the same landmark genes, that were used for fs for rna datan\n",
    "#for fs with prot data\n",
    "\n",
    "#find overlapping landmark genes and prot features\n",
    "overlapping_landmarks, _ = dp_nb.keep_overlapping(\n",
    "    pd.DataFrame(landmark_genes['Symbol']), prot.T)\n",
    "\n",
    "overlapping_landmarks = overlapping_landmarks['Symbol'].values\n",
    "\n",
    "#create prot data for all drugs\n",
    "x_all_prot, x_drug, y_list = dp_nb.create_all_drugs(\n",
    "    prot[overlapping_landmarks], one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "#fmt index to include drug cell line paris\n",
    "cls_drugs_index = x_all_prot.index + '::' + x_drug.index \n",
    "x_all_prot.index = cls_drugs_index\n",
    "y_list.index = cls_drugs_index\n",
    "x_drug.index = cls_drugs_index\n",
    "\n",
    "x_all_prot = x_all_prot.astype(np.float32)\n",
    "x_all_prot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11583, 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot data creation for all drugs\n",
    "x_hot, x_drug_hot, y_hot = dp_nb.create_all_drugs(\n",
    "    one_hot_cls, one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "cls_drugs_index_hot = x_hot.index + '::' + x_drug_hot.index \n",
    "\n",
    "x_hot.index = cls_drugs_index_hot\n",
    "x_hot.columns = np.arange(len(x_drug.columns), len(x_drug.columns) + len(x_hot.columns))\n",
    "x_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_shape=None\n",
    "def build_cnn_kt(hp):\n",
    "    if _input_shape == None:\n",
    "        raise Exception('add input shape dim')\n",
    "    phos_input = layers.Input(shape=(_input_shape, 1))\n",
    "    x = layers.Conv1D(\n",
    "        filters=hp.Int('filts', 8, 32, 8), kernel_size=16, \n",
    "        activation='relu')(phos_input)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(\n",
    "        filters=hp.Int('filts',8, 32, 8), kernel_size=8, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hp.Int('units', 32, 258, 32), activation='relu')(x)\n",
    "    x = layers.Dense(hp.Int('units', 32, 258, 32), activation='relu')(x)\n",
    "    drug_input = layers.Input(shape = (xdrug_train.shape[1]))\n",
    "    concatenated = layers.concatenate([x, drug_input])\n",
    "    hidd = layers.Dense(hp.Int('units_hid', 32, 258, 32), activation='relu')(concatenated)\n",
    "    hidd = layers.Dense(hp.Int('units_hid', 32, 258, 32), activation='relu')(hidd)\n",
    "    output_tensor = layers.Dense(1)(hidd)\n",
    "    model = tf.keras.Model([phos_input,drug_input], output_tensor)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=hp.Choice('lr', [1e-4, 1e-3]))\n",
    "    model.compile(optimizer=opt, loss=tf.keras.metrics.mean_squared_error, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    4,   11,   26,   64,   70,   77,   85,   93,  103,  113,\n",
       "        125,  137,  151,  166,  183,  201,  221,  244,  268,  295,  325,\n",
       "        358,  394,  433,  477,  525,  577,  635,  699,  769,  847,  932,\n",
       "       1025, 1128, 1242, 1366, 1503, 1654, 1821, 2003, 2205, 2426, 2669,\n",
       "       2937, 3232, 3557, 3914, 4307, 4740, 5215, 5739, 6315, 6949])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_size  = 0.6 #train size relative to total data set size\n",
    "lg_space = np.logspace(1, np.log2(64), base=2.0, num=5).astype(int)\n",
    "lg2 = np.logspace(np.log2(64), np.log2(len(x_all) * _train_size),  base=2.0, num=50).astype(int)\n",
    "lg_space = np.concatenate((lg_space, lg2))\n",
    "lg_space = np.unique(lg_space)\n",
    "lg_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 03s]\n",
      "val_loss: 5.201609134674072\n",
      "\n",
      "Best val_loss So Far: 5.201609134674072\n",
      "Total elapsed time: 00h 00m 44s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "filts             |16                |24                \n",
      "units             |160               |224               \n",
      "units_hid         |96                |224               \n",
      "lr                |0.0001            |0.001             \n",
      "\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 1s 72ms/step - loss: 9.4728 - mae: 2.6773 - val_loss: 9.4872 - val_mae: 2.6195\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 9.3842 - mae: 2.6626 - val_loss: 9.4256 - val_mae: 2.6092\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 9.3195 - mae: 2.6515 - val_loss: 9.3720 - val_mae: 2.6001\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.2594 - mae: 2.6418 - val_loss: 9.3204 - val_mae: 2.5913\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.2003 - mae: 2.6316 - val_loss: 9.2548 - val_mae: 2.5799\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 9.1289 - mae: 2.6190 - val_loss: 9.1956 - val_mae: 2.5695\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 9.0607 - mae: 2.6067 - val_loss: 9.1270 - val_mae: 2.5574\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.9838 - mae: 2.5932 - val_loss: 9.0564 - val_mae: 2.5449\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.9044 - mae: 2.5785 - val_loss: 8.9839 - val_mae: 2.5319\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 8.8228 - mae: 2.5641 - val_loss: 8.8965 - val_mae: 2.5160\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 8.7262 - mae: 2.5458 - val_loss: 8.8048 - val_mae: 2.4990\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 8.6228 - mae: 2.5266 - val_loss: 8.6955 - val_mae: 2.4781\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 8.5009 - mae: 2.5026 - val_loss: 8.5953 - val_mae: 2.4585\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 8.3849 - mae: 2.4786 - val_loss: 8.4727 - val_mae: 2.4334\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 8.2462 - mae: 2.4491 - val_loss: 8.3612 - val_mae: 2.4099\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 8.1183 - mae: 2.4214 - val_loss: 8.2361 - val_mae: 2.3824\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 7.9708 - mae: 2.3886 - val_loss: 8.1002 - val_mae: 2.3505\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.8139 - mae: 2.3484 - val_loss: 7.9799 - val_mae: 2.3203\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.6733 - mae: 2.3087 - val_loss: 7.9231 - val_mae: 2.3062\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7.5985 - mae: 2.2930 - val_loss: 7.8336 - val_mae: 2.2805\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.4806 - mae: 2.2571 - val_loss: 7.7649 - val_mae: 2.2579\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7.3841 - mae: 2.2236 - val_loss: 7.7337 - val_mae: 2.2491\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 7.3338 - mae: 2.2124 - val_loss: 7.6885 - val_mae: 2.2336\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 7.2693 - mae: 2.1869 - val_loss: 7.6620 - val_mae: 2.2252\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7.2187 - mae: 2.1781 - val_loss: 7.6264 - val_mae: 2.2076\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.1575 - mae: 2.1486 - val_loss: 7.6024 - val_mae: 2.2037\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.1315 - mae: 2.1449 - val_loss: 7.5796 - val_mae: 2.1952\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 7.0872 - mae: 2.1292 - val_loss: 7.5499 - val_mae: 2.1885\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 7.0503 - mae: 2.1230 - val_loss: 7.5254 - val_mae: 2.1774\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 7.0167 - mae: 2.1069 - val_loss: 7.5141 - val_mae: 2.1664\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.9599 - mae: 2.0845 - val_loss: 7.4791 - val_mae: 2.1613\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.9260 - mae: 2.0805 - val_loss: 7.4589 - val_mae: 2.1590\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.9034 - mae: 2.0799 - val_loss: 7.4530 - val_mae: 2.1485\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.8699 - mae: 2.0568 - val_loss: 7.4385 - val_mae: 2.1423\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.8334 - mae: 2.0498 - val_loss: 7.4026 - val_mae: 2.1378\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.8001 - mae: 2.0419 - val_loss: 7.3636 - val_mae: 2.1393\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 6.7758 - mae: 2.0452 - val_loss: 7.3564 - val_mae: 2.1323\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.7433 - mae: 2.0322 - val_loss: 7.3304 - val_mae: 2.1309\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.7179 - mae: 2.0355 - val_loss: 7.3285 - val_mae: 2.1221\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.6930 - mae: 2.0191 - val_loss: 7.3250 - val_mae: 2.1162\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 6.6700 - mae: 2.0026 - val_loss: 7.3026 - val_mae: 2.1110\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.6360 - mae: 1.9967 - val_loss: 7.2839 - val_mae: 2.1081\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.6056 - mae: 1.9933 - val_loss: 7.2413 - val_mae: 2.1072\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.5739 - mae: 1.9973 - val_loss: 7.2543 - val_mae: 2.0980\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 6.5607 - mae: 1.9818 - val_loss: 7.2534 - val_mae: 2.0948\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.5282 - mae: 1.9684 - val_loss: 7.2052 - val_mae: 2.0938\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.5063 - mae: 1.9728 - val_loss: 7.2177 - val_mae: 2.0885\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.4799 - mae: 1.9565 - val_loss: 7.1928 - val_mae: 2.0873\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 6.4631 - mae: 1.9567 - val_loss: 7.1958 - val_mae: 2.0832\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.4319 - mae: 1.9434 - val_loss: 7.1377 - val_mae: 2.0849\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 6.3978 - mae: 1.9534 - val_loss: 7.1570 - val_mae: 2.0772\n",
      "Epoch 52/100\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5.6427 - mae: 1.7137"
     ]
    }
   ],
   "source": [
    "#One hot\n",
    "#finds a test train split then finds the learning curve\n",
    "#for that split. Repeats for mutiple (N) test train splits \n",
    "N = 30\n",
    "t1 = time.time()\n",
    "for run in range(N):\n",
    "    print(f'run {run} of {N}')\n",
    "    #test train split\n",
    "    rand_seed = 42 + run\n",
    "    pairs_with_truth_vals =  y_list.index\n",
    "    train_pairs, test_pairs, val_pairs = tts_nb.split(\n",
    "        rand_seed, _all_cls, _all_drugs, pairs_with_truth_vals,\n",
    "        train_size=_train_size)\n",
    "\n",
    "    #rna test train selection\n",
    "    x_train_rna, x_test_rna = x_all.loc[train_pairs], x_all.loc[test_pairs]\n",
    "    x_val_rna = x_all.loc[val_pairs]\n",
    "    y_train, y_test = y_list[train_pairs], y_list[test_pairs]\n",
    "    y_val = y_list[val_pairs]\n",
    "    xdrug_train, xdrug_test = x_drug.loc[train_pairs], x_drug.loc[test_pairs]\n",
    "    xdrug_val = x_drug.loc[val_pairs]\n",
    "\n",
    "    #prot test train selection\n",
    "    x_train_prot, x_test_prot = x_all_prot.loc[train_pairs], x_all_prot.loc[test_pairs]\n",
    "    x_val_prot = x_all_prot.loc[val_pairs]\n",
    "\n",
    "    #one hot test train seleciton\n",
    "    x_train_hot, x_test_hot = x_hot.loc[train_pairs], x_hot.loc[test_pairs]\n",
    "    x_val_hot = x_hot.loc[val_pairs]\n",
    "    \n",
    "    #consistencey checks\n",
    "    assert (x_train_hot.index == x_train_rna.index).all()\n",
    "    assert (x_test_hot.index == x_test_rna.index).all()\n",
    "    assert (x_val_hot.index == x_val_rna.index).all()\n",
    "\n",
    "    assert (x_train_prot.index == x_train_rna.index).all()\n",
    "    assert (x_test_prot.index == x_test_rna.index).all()\n",
    "    assert (x_val_prot.index == x_val_rna.index).all()\n",
    "\n",
    "    assert (y_train.index == x_train_rna.index).all()\n",
    "    assert (y_test.index == x_test_rna.index).all()\n",
    "    assert (xdrug_test.index == x_test_rna.index).all()\n",
    "\n",
    "    #inconsistencey checks\n",
    "    assert x_train_rna.shape[1] != x_train_prot.shape[1]\n",
    "    assert x_test_rna.shape[1] != x_test_prot.shape[1]\n",
    "    assert x_val_rna.shape[1] != x_val_prot.shape[1]\n",
    "\n",
    "    assert x_train_rna.shape[1] != x_train_hot.shape[1]\n",
    "    assert x_test_rna.shape[1] != x_test_hot.shape[1]\n",
    "    assert x_val_rna.shape[1] != x_val_hot.shape[1]\n",
    "\n",
    "    assert x_train_prot.shape[1] != x_train_hot.shape[1]\n",
    "    assert x_test_prot.shape[1] != x_test_hot.shape[1]\n",
    "\n",
    "    del x_train_rna, x_val_rna, x_test_rna\n",
    "    del x_train_prot, x_val_prot, x_test_prot\n",
    "    \n",
    "    data_type = 'One-hot'\n",
    "    #run the learning curve\n",
    "    _input_shape = x_train_hot.shape[1]\n",
    "    mse_r2, bms, bhps = lc_nb.run_lc_ucl(\n",
    "        build_cnn_kt,\n",
    "        [x_train_hot, xdrug_train], \n",
    "        y_train, \n",
    "        [x_val_hot, xdrug_val], \n",
    "        y_val, \n",
    "        [x_test_hot, xdrug_test],\n",
    "        y_test, \n",
    "        lg_space, \n",
    "        num_trails=15,\n",
    "        epochs=100,\n",
    "        direc='UCL-del2')\n",
    "    \n",
    "    #save data\n",
    "    mse_r2.to_csv(f'LC-metric-results/{data_type}/run{run}')\n",
    "    \n",
    "    bhps_df = pd.DataFrame([bhp.values for bhp in bhps])\n",
    "    bhps_df.to_csv(f'Optimal-hyperparameters/{data_type}/run{run}df')\n",
    "    with open(f'Optimal-hyperparameters/{data_type}/run{run}.pkl', 'wb') as f:\n",
    "        pickle.dump(bhps, f)\n",
    "        \n",
    "    model_path = f'optimal-models{data_type}/run{run}/model_train_size_'\n",
    "    for train_size, model in zip(lg_space, bms):\n",
    "        model.save(model_path + str(train_size)) \n",
    "        \n",
    "    np.savetxt(f'train_test_inds/{data_type}/train_inds{run}', y_train.index, fmt='%s')\n",
    "    np.savetxt(f'train_test_inds/{data_type}/test_inds{run}', y_test.index, fmt='%s')\n",
    "    np.savetxt(f'train_test_inds/{data_type}/val_inds{run}', y_val.index, fmt='%s')\n",
    "    \n",
    "delt = time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPUforge",
   "language": "python",
   "name": "tfgpuforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
