{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "import pickle\n",
    "from importlib import reload\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from collections import namedtuple\n",
    "import keras_tuner as kt \n",
    "codebase_path = '/data/home/wpw035/Codebase'\n",
    "sys.path.insert(0, codebase_path) #add path to my codebase models\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DRP_utils import data_preprocessing as dp_nb\n",
    "reload(dp_nb)\n",
    "from DRP_utils import model_selection as ms_nb\n",
    "reload(ms_nb)\n",
    "from DRP_utils import testing as t_nb\n",
    "reload(t_nb)\n",
    "import Data_imports as di_nb\n",
    "reload(di_nb)\n",
    "import pairs_train_test_split as tts_nb\n",
    "import Learning_curve as lc_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/wpw035/.conda/envs/tfGPUforge/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing prot values 0.386335609896865\n",
      "num non overlapping prot and target cls: 10\n",
      "num non overlapping cls: 930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((38, 8457), (38, 17417), (38, 22804), (38, 38), (345, 345))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input data\n",
    "prot, rna, phospho_ls, one_hot_cls, one_hot_drugs, ic50_df1 = di_nb.read_input_data()\n",
    "_all_cls = prot.index\n",
    "_all_drugs = ic50_df1.columns\n",
    "assert prot.shape[0] == rna.shape[0] == phospho_ls.shape[0]\n",
    "assert phospho_ls.shape[0]  == one_hot_cls.shape[0]\n",
    "prot.shape, rna.shape, phospho_ls.shape, one_hot_cls.shape, one_hot_drugs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featrue selection (FS) and data createing for each drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11583, 908), (11583, 345), 11583)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in landmark genes for fs and find landmarks that overlap with rna data\n",
    "landmark_genes = pd.read_csv(\n",
    "    f'{codebase_path}/downloaded_data_small/landmark_genes_LINCS.txt',sep='\\t')\n",
    "landmark_genes.index = landmark_genes['Symbol']\n",
    "\n",
    "overlapping_landmarks, _ = dp_nb.keep_overlapping(\n",
    "    pd.DataFrame(landmark_genes['Symbol']), rna.T)\n",
    "\n",
    "overlapping_landmarks = overlapping_landmarks['Symbol'].values\n",
    "\n",
    "#create input data for each drug\n",
    "x_all, x_drug, y_list = dp_nb.create_all_drugs(\n",
    "    rna[overlapping_landmarks], one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "x_all = x_all.astype(np.float32)\n",
    "x_drug = x_drug.astype(np.float16)\n",
    "\n",
    "#fmt index to include drug cell line paris\n",
    "cls_drugs_index = x_all.index + '::' + x_drug.index\n",
    "x_all.index = cls_drugs_index\n",
    "x_drug.index = cls_drugs_index\n",
    "y_list.index = cls_drugs_index\n",
    "\n",
    "x_all.shape, x_drug.shape, len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11583, 721)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the same landmark genes, that were used for fs for rna datan\n",
    "#for fs with prot data\n",
    "\n",
    "#find overlapping landmark genes and prot features\n",
    "overlapping_landmarks, _ = dp_nb.keep_overlapping(\n",
    "    pd.DataFrame(landmark_genes['Symbol']), prot.T)\n",
    "\n",
    "overlapping_landmarks = overlapping_landmarks['Symbol'].values\n",
    "\n",
    "#create prot data for all drugs\n",
    "x_all_prot, x_drug, y_list = dp_nb.create_all_drugs(\n",
    "    prot[overlapping_landmarks], one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "#fmt index to include drug cell line paris\n",
    "cls_drugs_index = x_all_prot.index + '::' + x_drug.index \n",
    "x_all_prot.index = cls_drugs_index\n",
    "y_list.index = cls_drugs_index\n",
    "x_drug.index = cls_drugs_index\n",
    "\n",
    "x_all_prot = x_all_prot.astype(np.float32)\n",
    "x_all_prot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11583, 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot data creation for all drugs\n",
    "x_hot, x_drug_hot, y_hot = dp_nb.create_all_drugs(\n",
    "    one_hot_cls, one_hot_drugs, ic50_df1, _all_cls)\n",
    "\n",
    "cls_drugs_index_hot = x_hot.index + '::' + x_drug_hot.index \n",
    "\n",
    "x_hot.index = cls_drugs_index_hot\n",
    "x_hot.columns = np.arange(len(x_drug.columns), len(x_drug.columns) + len(x_hot.columns))\n",
    "x_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input_shape=None\n",
    "def build_cnn_kt(hp):\n",
    "    if _input_shape == None:\n",
    "        raise Exception('add input shape dim')\n",
    "    phos_input = layers.Input(shape=(_input_shape, 1))\n",
    "    x = layers.Conv1D(\n",
    "        filters=hp.Int('filts', 8, 32, 8), kernel_size=16, \n",
    "        activation='relu')(phos_input)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(\n",
    "        filters=hp.Int('filts',8, 32, 8), kernel_size=8, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(hp.Int('units', 32, 258, 32), activation='relu')(x)\n",
    "    x = layers.Dense(hp.Int('units', 32, 258, 32), activation='relu')(x)\n",
    "    drug_input = layers.Input(shape = (xdrug_train.shape[1]))\n",
    "    concatenated = layers.concatenate([x, drug_input])\n",
    "    hidd = layers.Dense(hp.Int('units_hid', 32, 258, 32), activation='relu')(concatenated)\n",
    "    hidd = layers.Dense(hp.Int('units_hid', 32, 258, 32), activation='relu')(hidd)\n",
    "    output_tensor = layers.Dense(1)(hidd)\n",
    "    model = tf.keras.Model([phos_input,drug_input], output_tensor)\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=hp.Choice('lr', [1e-4, 1e-3]))\n",
    "    model.compile(optimizer=opt, loss=tf.keras.metrics.mean_squared_error, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    4,   11,   26,   64,   70,   77,   85,   93,  103,  113,\n",
       "        125,  137,  151,  166,  183,  201,  221,  244,  268,  295,  325,\n",
       "        358,  394,  433,  477,  525,  577,  635,  699,  769,  847,  932,\n",
       "       1025, 1128, 1242, 1366, 1503, 1654, 1821, 2003, 2205, 2426, 2669,\n",
       "       2937, 3232, 3557, 3914, 4307, 4740, 5215, 5739, 6315, 6949])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train_size  = 0.6 #train size relative to total data set size\n",
    "lg_space = np.logspace(1, np.log2(64), base=2.0, num=5).astype(int)\n",
    "lg2 = np.logspace(np.log2(64), np.log2(len(x_all) * _train_size),  base=2.0, num=50).astype(int)\n",
    "lg_space = np.concatenate((lg_space, lg2))\n",
    "lg_space = np.unique(lg_space)\n",
    "lg_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 00m 24s]\n",
      "val_loss: 1.6598126888275146\n",
      "\n",
      "Best val_loss So Far: 1.4955540895462036\n",
      "Total elapsed time: 00h 02m 38s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "filts             |16                |16                \n",
      "units             |192               |192               \n",
      "units_hid         |32                |192               \n",
      "lr                |0.001             |0.001             \n",
      "\n",
      "Epoch 1/100\n",
      "107/107 [==============================] - 1s 8ms/step - loss: 7.1380 - mae: 2.0869 - val_loss: 6.9492 - val_mae: 1.9350\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 1s 6ms/step - loss: 6.2024 - mae: 1.9407 - val_loss: 5.7929 - val_mae: 1.7684\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 1s 6ms/step - loss: 4.9592 - mae: 1.6996 - val_loss: 3.8245 - val_mae: 1.4711\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 3.1645 - mae: 1.3094 - val_loss: 2.4861 - val_mae: 1.1342\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 2.1877 - mae: 1.0915 - val_loss: 2.1632 - val_mae: 1.0644\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 1s 7ms/step - loss: 1.8667 - mae: 1.0069 - val_loss: 2.0398 - val_mae: 1.0492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finds a test train split then finds the learning curve\n",
    "#for that split. Repeats for mutiple (N) test train splits \n",
    "N = 30\n",
    "t1 = time.time()\n",
    "for run in range(N):\n",
    "    print(f'run {run} of {N}')\n",
    "    #test train split\n",
    "    rand_seed = 42 + run\n",
    "    pairs_with_truth_vals =  y_list.index\n",
    "    train_pairs, test_pairs, val_pairs = tts_nb.split(\n",
    "        rand_seed, _all_cls, _all_drugs, pairs_with_truth_vals,\n",
    "        train_size=_train_size)\n",
    "\n",
    "    #rna test train selection\n",
    "    x_train_rna, x_test_rna = x_all.loc[train_pairs], x_all.loc[test_pairs]\n",
    "    x_val_rna = x_all.loc[val_pairs]\n",
    "    y_train, y_test = y_list[train_pairs], y_list[test_pairs]\n",
    "    y_val = y_list[val_pairs]\n",
    "    xdrug_train, xdrug_test = x_drug.loc[train_pairs], x_drug.loc[test_pairs]\n",
    "    xdrug_val = x_drug.loc[val_pairs]\n",
    "\n",
    "    #prot test train selection\n",
    "    x_train_prot, x_test_prot = x_all_prot.loc[train_pairs], x_all_prot.loc[test_pairs]\n",
    "    x_val_prot = x_all_prot.loc[val_pairs]\n",
    "\n",
    "    #one hot test train seleciton\n",
    "    x_train_hot, x_test_hot = x_hot.loc[train_pairs], x_hot.loc[test_pairs]\n",
    "    x_val_hot = x_hot.loc[val_pairs]\n",
    "    \n",
    "    #consistencey checks\n",
    "    assert (x_train_hot.index == x_train_rna.index).all()\n",
    "    assert (x_test_hot.index == x_test_rna.index).all()\n",
    "    assert (x_val_hot.index == x_val_rna.index).all()\n",
    "\n",
    "    assert (x_train_prot.index == x_train_rna.index).all()\n",
    "    assert (x_test_prot.index == x_test_rna.index).all()\n",
    "    assert (x_val_prot.index == x_val_rna.index).all()\n",
    "\n",
    "    assert (y_train.index == x_train_rna.index).all()\n",
    "    assert (y_test.index == x_test_rna.index).all()\n",
    "    assert (xdrug_test.index == x_test_rna.index).all()\n",
    "\n",
    "    #inconsistencey checks\n",
    "    assert x_train_rna.shape[1] != x_train_prot.shape[1]\n",
    "    assert x_test_rna.shape[1] != x_test_prot.shape[1]\n",
    "    assert x_val_rna.shape[1] != x_val_prot.shape[1]\n",
    "\n",
    "    assert x_train_rna.shape[1] != x_train_hot.shape[1]\n",
    "    assert x_test_rna.shape[1] != x_test_hot.shape[1]\n",
    "    assert x_val_rna.shape[1] != x_val_hot.shape[1]\n",
    "\n",
    "    assert x_train_prot.shape[1] != x_train_hot.shape[1]\n",
    "    assert x_test_prot.shape[1] != x_test_hot.shape[1]\n",
    "    \n",
    "    del x_train_rna, x_test_rna, x_val_rna\n",
    "    del x_train_hot, x_test_hot, x_val_hot\n",
    "    \n",
    "    #---------- Make sure correct dtype -----------------\n",
    "    data_type = 'Prot'    \n",
    "    #run the learning curve\n",
    "    _input_shape = x_train_prot.shape[1]\n",
    "    mse_r2, bms, bhps = lc_nb.run_lc_ucl(\n",
    "        build_cnn_kt,\n",
    "        [x_train_prot, xdrug_train], \n",
    "        y_train, \n",
    "        [x_val_prot, xdrug_val], \n",
    "        y_val, \n",
    "        [x_test_prot, xdrug_test],\n",
    "        y_test, \n",
    "        lg_space, \n",
    "        num_trails=15,\n",
    "        epochs=100,\n",
    "        direc='UCL-del3')\n",
    "    \n",
    "    #save data\n",
    "    mse_r2.to_csv(f'LC-metric-results/{data_type}/run{run}')\n",
    "    \n",
    "    bhps_df = pd.DataFrame([bhp.values for bhp in bhps])\n",
    "    bhps_df.to_csv(f'Optimal-hyperparameters/{data_type}/run{run}df')\n",
    "    with open(f'Optimal-hyperparameters/{data_type}/run{run}.pkl', 'wb') as f:\n",
    "        pickle.dump(bhps, f)\n",
    "        \n",
    "    scratch_path = '/data/scratch/wpw035/In-use/DRP/rna-phos-intersect-models'\n",
    "    model_path = f'{scratch_path}/NN-models/{data_type}/run{run}model_train_size_'\n",
    "    \n",
    "    for train_size, model in zip(lg_space, bms):\n",
    "        model.save(model_path + str(train_size)) \n",
    "        \n",
    "    np.savetxt(f'train_test_inds/{data_type}/train_inds{run}', y_train.index, fmt='%s')\n",
    "    np.savetxt(f'train_test_inds/{data_type}/test_inds{run}', y_test.index, fmt='%s')\n",
    "    np.savetxt(f'train_test_inds/{data_type}/val_inds{run}', y_val.index, fmt='%s')\n",
    "    \n",
    "delt = time.time() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPUforge",
   "language": "python",
   "name": "tfgpuforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
